---
title: "Untitled"
author: "HW4"
date: "2025-02-14"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include = FALSE}

# Set up the default parameters
# 1. The code block will not be shown in the document
# 2. set up figure display size
# 3. turn off all the warnings and messages

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 4)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```

# Background

For this data analysis, you will analyze the daily and weekly domestic passenger count arriving in Hawaii airports. File *DailyDomestic.csv* contains the *daily* number of passengers between May 2019 and February 2023 File *WeeklyDomestic.csv* contains the *weekly* number of passengers for the same time period. Here we will use different ways of fitting the ARIMA model while dealing with trend and seasonality.

```{r library}

library(lubridate)
library(mgcv)
library(tseries)
library(car)

```

# Instructions on reading the data

To read the data in `R`, save the file in your working directory (make sure you have changed the directory if different from the R working directory) and read the data using the `R` function `read.csv()`

```{r load data}
daily <- read.csv("DailyDomestic.csv", head = TRUE)
daily$date <- as.Date(daily$date)
weekly <- read.csv("WeeklyDomestic.csv", head = TRUE)
weekly$week <- as.Date(weekly$week)

```

# Question 1. Trend and seasonality estimation

**1a.** Plot the daily and weekly domestic passenger count separately. Do you see a strong trend and seasonality?

```{r}
# plot daily passenger count
head(daily)
# extract data in count column into a list vector, then fit a trend based on the list using mgcv package, hich is used for spline trend.
daily.gam.fit = gam(daily$domestic~s(c(1:length(daily$domestic))))
daily.trend = fitted(daily.gam.fit)
# plot counts and its trend.
daily.plot = plot(daily$date, daily$domestic, xlab="daily", ylab="daily count", type = "l")
lines(daily$date, daily.trend, lwd = 2, col="red")

#plot weekly passenger count, same processure as above
head(weekly)
#str(weekly)
weekly.gam.fit = gam(weekly$domestic~s(c(1:length(weekly$domestic))))
weekly.trend = fitted(weekly.gam.fit)
weekly.plot = plot(weekly$week, weekly$domestic, xlab="week", ylab="weekly count", type="l")
lines(weekly$week, weekly.trend, lwd = 2, col ="green")
```

*Response*
From both plots, there is a strong cyclic pattern in both plots, but not an obvious seasonal pattern. About trend, there may exist a trend, but not very evident. 


**1b.** (Trend and seasonality) Fit the *weekly* domestic passenger count with a non-parametric trend using splines and monthly seasonality using ANOVA. Is the seasonality significant? Plot the fitted values together with the original time series. Plot the residuals and the ACF of the residuals. Comment on how the model fits and on the appropriateness of the stationarity assumption of the residuals.

```{r non-parametric trend (weekly)}
#spline trend on weekly data.s means smooth function which is executed by gam()
weekly.gam.fit = gam(weekly$domestic~s(c(1:length(weekly$domestic))))

# find fitted value based on the trend
weekly.trend = fitted(weekly.gam.fit)

# fit monthly seasonality using anova. first, need to convert weekly date into month factors.then fit it into lm regression, then use anova to test if the null hyporthesis is correct. 
weekly.month = as.factor(format(weekly$week, "%b"))
month.season = gam(weekly$domestic~weekly.month)
anova(month.season)

#plot original data and fitted value.
plot(weekly$week, weekly$domestic, col="blue", type ="l", xlab="Time", ylab = "weekly count")
lines(weekly$week, fitted(month.season), col="red", lwd = 1)
legend("topleft", legend = c("origianl weekly count", "fitted value"), lwd=1, col=c("blue", "red"))

#plot residuals
plot(weekly$week, residuals(month.season), type ="l", lwd =1, xlab="time", ylab="residuals")

#plot acf of residuals
acf(residuals(month.season), lag.max = 100)

```

*Response:*

Based on anova result, the p value is very small , close to 0, indicating seasonality based on month has improved the prediction power of the model. and it suggests a seasonality overall and a trend, but relatively very small. the residual plot shows some pattern overall, for examples, there are spikes and troughs in some slightly "regular" way, the acf of residual shows some lags' value is outside confidence band, indicating the residual is not stationary due to seasonality.  



**1c.** (Trend and seasonality) This time fit the *daily* domestic passenger count with a non-parametric trend using splines, monthly and day-of-the-week seasonality using ANOVA. Plot the fitted values together with the original time series. Are the seasonal effects significant? Plot the residuals and the ACF of the residuals. Comment on how the model fits and on the appropriateness of the stationarity assumption of the residuals.

```{r non-parametric trend (daily)}
#spline trend on daily data.s means smooth function which is executed by gam()
daily.gam.fit = gam(daily$domestic~s(c(1:length(daily$domestic))))

# find fitted value based on the trend
daily.trend = fitted(daily.gam.fit)

# fit monthly seasonality and day of the week seasonality using anova.  
daily.month = as.factor(format(daily$date,"%b"))
day.of.week = as.factor(weekdays(daily$date))

#fit the model with month and day of the week.
month.daily.season = gam(daily$domestic~daily.month  + day.of.week)

#using anova to check the performance of the model
anova(month.daily.season)

#plot original data and fitted value.
plot(daily$date, daily$domestic, col="blue", type ="l", xlab="Time", ylab = "daily count")
lines(daily$date, fitted(month.daily.season), col="red", lwd = 1)
legend("topleft", legend = c("origianl daily count", "fitted value"), lwd=1, col=c("blue", "red"))

#plot residuals
plot(daily$date, residuals(month.daily.season), type ="l", lwd =1, xlab="time", ylab="residuals")

#plot acf of residuals
acf(residuals(month.daily.season), lag.max = 600)

```

*Response:*

From result of anova, month and day of week both have very low p value, suggesting they are statistically significant. from the plot, I can see a strong pattern, which suggests a seasonality. the residual plot suggests a strong variability in the data, acf of residual shows a similar pattern, and most of the lags are outside of confidence band, from overall pattern in acf, i can see a strong seasonality.  




# Question 2. ARMA fitting and residual analysis

**2a.** (ARMA fitting) Fit a ARMA model with both AR and MA orders of 6 without intercept using the residual processes from Question 1b and 1c for the daily and weekly domestic passenger count, respectively. What are the coefficients of the fitted models? Are the fitted ARMA models causal? (Hint: Set include.mean = FALSE if using arima(). Use polyroot() to find the roots of a polynomial.)

```{r ARMA fitting}
# fit arma model using arima function on daily data, since it should be both ar and ma, then give order c(6,0,6)
daily.arma = arima(residuals(month.daily.season), order = c(6,0,6), method = "ML", include.mean = FALSE)
# display coefficients
daily.arma

# fit arma model using arima function on weekly data, since it should be both ar and ma, then give order c(6,0,6)
weekly.arma = arima(residuals(month.season), order = c(6,0,6), method = "ML", include.mean = FALSE)
# display coefficients
weekly.arma

# using zeros to check casuality. Mod() is used to find modulus to see if it is on unit circle. 1 in c() is the coefficient of very first Xt.
round(Mod(polyroot(c(1, month.daily.season$coefficients[1:6]))), 3)

round(Mod(polyroot(c(1, month.season$coefficients[1:6]))), 3)

```

*Response* 
for weekly data, the coefficients are:
X(t) _ 0.8474X(t_1) _ 0.7465X(t_2) + 0.8091X(t_3) + 0.1250X(t_4) + 0.7143X(t_5) + 0.3948X(t_6) = Z(t) _ 0.4225Z(t_1) + 1.1811Z(t_2) + 0.3485Z(t_3) _ 0.4802Z(t_4) + 0.4261Z(t_5) _ 0.2908Z(t_6)

for daily data, the coefficents are:
X(t) + 0.3565X(t_1) _ 0.5897X(t_2) + 0.1171X(t_3) + 0.8214X(t_4) _ 0.0698X(t_5) _ 0.7964X(t_6) = Z(t) _ 0.6431(t_1) + 0.246Z(t_2) + 0.3485Z(t_3) _ 0.9491Z(t_4) _ 0.2790Z(t_5) _ 0.5356Z(t_6)

since some root are inside of unit circle, so the arma process is not causal.




**2b.** (Residual analysis) Plot the residual processes of the two fitted models in Question 2a. Display the ACF, PACF, QQ-plot of these residual processes. Do the residual processes satisfies the assumptions of the R implementation?

```{r residual analysis and plot}
# plot of residual, acf of residual, pacf of residual, qqplot of residual --- daily data
plot(daily$date, residuals(daily.arma), xlab="time", ylab="daily residual", type = "l")
acf(residuals(daily.arma), lag.max = 600)
pacf(residuals(daily.arma), lag.max = 600)
qqPlot(residuals(daily.arma), ylab="daily residuals")

# plot of residual, acf of residual, pacf of residual, qqplot of residual --- weekly data
plot(weekly$week, residuals(weekly.arma), xlab="time", ylab="weekly residual", type = "l")
acf(residuals(weekly.arma), lag.max = 60)
pacf(residuals(weekly.arma), lag.max = 60)
qqPlot(residuals(weekly.arma), ylab="weekly residuals")



```

*Response* 

In arma model with trend and seasonality, acf, pacf plots suggest correlation within the weekly and daily data, there are some values are outside of pacf confidence band. QQplot suggest it in general can be considered as normal distribution even though at both tails diverge apart from the line.





# Question 3. ARMA fitting and model selection: Differenced daily domestic passenger count

**3a.** (Differencing for seasonality) Difference the daily domestic passenger count by 7 days, then again by 365 days. Plot the differenced time series, its ACF and PACF. Does this looks like a pure AR/MA process from the ACF/PACF plot?

```{r differencing for seasonality}
daily.differencing.seven = diff(daily$domestic, 7)
daily.differencing.year = diff(daily.differencing.seven, 365)

ts.plot(daily.differencing.seven, ylab = "differencing daily count by 7")
ts.plot(daily.differencing.year, ylab = "differencing daily count by 365")

acf(daily.differencing.year, lag.max = 200)

pacf(daily.differencing.year, lag.max = 200)

```

*Response:* 
it is not a pure ar process since the pacf does not cut off. I would say ar(6) process because acf cuts off when lags is 6 and pacf is gradually tails off. 




**3b.** (ARMA fitting and order selection). Fit an ARMA model without intercept using the differenced daily data with AR and MA order up to 8. Select the best ARMA model using AICc. What is the order for the selected model and what is its AICc?

```{r ARMA model}
# reference lecture note
# chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://www.cliffsnotes.com/api/v1/documents/download/6736870/

n = length(daily.differencing.year)
norder = 8
p = c(1:norder)-1
q = c(1:norder)-1

aic = matrix(0, norder, norder)

for (i in 1:norder) {
  for (j in 1:norder) {
    modij = arima(daily.differencing.year, order=c(p[i], 0, q[j]), method = "ML", include.mean = FALSE)
    aic[i, j] = modij$aic + 2 * (p[i] + q[j] +1) * (p[i] + q[j])/(n - p[i] - q[j] - 1)
  } 
}

indexp = rep(c(1:norder),norder)
indexq = rep(c(1:norder),each=norder)
indexaic = which(aic == min(aic))
porder = indexp[indexaic]-1
qorder = indexq[indexaic]-1
porder 
qorder

best.arma = arima(daily.differencing.year, order = c(porder, 0, qorder), method = "ML", include.mean = FALSE)
best.arma

```

*Response:* 
best porder and qorder is 3 and 7, so the best model is arma(3, 7), aic is 19357.43





**3c.** (Residual analysis) Plot the residual process of the selected model in Question 3b. Display the ACF, PACF and QQ plot of the residual process. How does this model compare to the one on daily passenger count data in Question 2b?

```{r residual analysis}

```

*Response:* 

**3d.** (Testing uncorrelated residuals) Use the Ljung-Box Test to decide whether the residuals of the selected ARMA model in Question 3b are correlated.

```{r Ljung-Box test}

```

*Response:* 






# Question 4. Seasonal ARMA model and forecasting: Weekly domestic passenger count

**4a.** (Seasonal ARMA) Use the first 185 data points of weekly domestic passenger count as training data. Fit a seasonal ARMA model with intercept, where the non-seasonal model is ARMA(1,1) and the seasonal model is AR(1) with a period of 52 weeks. Plot the residual process and the ACF of the residual process. Comment on the appropriateness of the fitted model.

```{r Seasonal ARMA}
training.data = 185
weekly.row = nrow(weekly)

weekly.arma.ar = arima(weekly$domestic[1:training.data], order=c(1,0,1), seasonal = list(order = c(1,0,0), period= 52), method = "ML")

weekly.arma.ar

plot(weekly$week[1:training.data], residuals(weekly.arma.ar), type = "l", ylab="weekly residuals")

acf(residuals(weekly.arma.ar), lag.max = 60)

```

*Response:* 

all lags fell inside the confidence band except first one, suggesting the model is a good fit. 




**4b.** (Forecasting) Use the fitted model in Question 4a to predict the total passenger count of the remainder of the weeks. Plot the 99% confidence interval. Compare with the actual observation. Does the actual observation fell into the 99% confidence interval?

```{r forecasting}

prediction = predict(weekly.arma.ar, n.ahead = weekly.row - training.data)

plot(weekly$week[training.data:weekly.row], weekly$domestic[training.data:weekly.row], type="l", xlab ="time", ylab="weekly count", ylim = c(80000, 220000))

points(weekly$week[(training.data+1):weekly.row], prediction$pred, col="red")

upperbound = prediction$pred + 2.58*prediction$se 
lowerbound = prediction$pred - 2.58*prediction$se

lines(weekly$week[(training.data+1):weekly.row], upperbound, col="blue")
lines(weekly$week[(training.data+1):weekly.row], lowerbound, col="blue")

legend("topleft", legend=c("weekly count", "predicted value", "confidence band"), lwd = 2, col =c("black", "red", "blue"))

actual.data = weekly$domestic[(training.data+1):weekly.row]

comparison = data.frame(actual.data, prediction$pred, upperbound, lowerbound)

print(comparison)

```

*Response* 
from the output, all actual data fall inside the 99% confidence band for the remaining weeks. i think it is because of 99% confidence band which is very wide, including large uncertainty and variability.  








