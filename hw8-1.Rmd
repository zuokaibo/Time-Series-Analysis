---
title: "hw8"
output: html_document
date: "2025-04-11"
---

```{r setup, include=FALSE}
# Set up the default parameters
# 1. The code block will be shown in the document
# 2. set up figure display size
# 3. turn off all the warnings and messages

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 4)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```
# Background

We have explored how various U.S. economic indicators are related to each other, which is a classic application for the VAR modeling. The U.S. Federal Reserve use these economic indicators to make monetary policy decision. Specifically, we will focus on one the top commodities, electricity prices (cents per kilowatt-hour (cents/kWh), for the top most metropolitan areas in the U.S. In this homework, we will analysis the average electricity prices for the following areas:

-   Washington DC
-   Atlanta, GA
-   Los Angeles, CA
-   Seattle, WA

The data covered for the analysis is the period starting from Jan 1998 - Dec 2023, in a monthly frequency.

# Instructions on reading the data

To read the data in R, save the file in your working directory (make sure you have changed the directory if different from the R working directory) and read the data using the R function read.csv()

```{r cars}
# Read in the monthly data

data <- read.csv('Metro_electricity_prices.csv', header = TRUE)
idx <- as.Date(as.character(data$DATE), '%m/%d/%y')

```


```{r pressure, echo=FALSE}
library(lubridate)
library(dplyr)
library(data.table)
library(vars)
library(xts)
library(mgcv)
library(stats)
library(tseries)
library(aod)
library(forecast)
library(ggplot2)
```
# Question 1: Univariate Analysis

## Question 1a

Plot the time series of all indicators for comparison and discuss whether you find any similarities in terms of trend or other features.
Plot also the 1st order difference overlay plot and the corresponding ACF plots for the 4 states. Comment on the original and differenced time series data in terms of stationarity and volatility.

```{r}
# transform variables into time series
dc = xts(data$DC, idx)
la = xts(data$LA, idx)
sea = xts(data$SEA, idx)
atl = xts(data$ATL, idx)

# merge all univariate time series into multivariate time series 
all.merge = merge(dc, la, join="inner")
all.merge = merge(all.merge, sea, join = "inner")
all.merge = merge(all.merge, atl, join = "inner")
colnames(all.merge) = c("tsdc", "tsla", "tssea", "tsatl")

#plot time series and its first order differencing
plot(all.merge, main="price of four states")

all.merge.diff = diff(all.merge)
plot(all.merge.diff, main = "price of four states with first order differencing")

#plot ACF 
dc.price.ts = ts(as.numeric(all.merge$tsdc), start=c(1998,1), frequency = 12)
dc.price.diff = diff(dc.price.ts)
acf(dc.price.diff, main="DC price")

la.price.ts = ts(as.numeric(all.merge$tsla), start=c(1998,1), frequency = 12)
la.price.diff = diff(la.price.ts)
acf(la.price.diff, main="LA price")

sea.price.ts = ts(as.numeric(all.merge$tssea), start=c(1998,1), frequency = 12)
sea.price.diff = diff(sea.price.ts)
acf(sea.price.diff, main="Seattle price")

atl.price.ts = ts(as.numeric(all.merge$tsatl), start=c(1998,1), frequency = 12)
atl.price.diff = diff(atl.price.ts)
acf(atl.price.diff, main="Atlanta price")

```

*Response*
From original data, all indicators have an increasing trend until Jan2009, then a sudden drop, but a general increasing trend again till 2014, a sudden drop happened again then increased back. we can learn that the variability have increased or decreased dramaticlly over the time, even worse since 2019 due to covid. From the 1st order differencing data, we have removed the trend, but a strong volatilties still exist. The ACF plots of all time series have show a seasonality. 



## Question 1b

Using the original, undifferenced data, divide the data into training & test datasets with the last 9 months of 2023 being the testing data: April 2023 - Dec 2023. Fit a non-parametric trend and seasonality (use monthly factor) using the splines regression on the DC rate training dataset. Comment on the effectiveness of the splines fit. Then, apply ARMA to the residuals obtained from this splines fitting. Use max order of 6. Evaluate goodness of fit for the ARMA model (plot residuals, ACF, PACF, and Q-Q plot & residual tests).

Lastly, fit the ARMA model on training data (Not residuals) with the optimal orders selected above. Forecast the last 9 months of 2023 (testing data) and compare to the observed values. Discuss why there are (or not!) significant differences between predicted vs observed. To do this, you should also evaluate the prediction intervals with a 95% confidence level.

```{r}
dc.train = dc.price.ts[0:(length(dc.price.ts) - 9)]
dc.test = dc.price.ts[(length(dc.price.ts) - 8) : length(dc.price.ts)]

dc.train.ts = ts(dc.train, start = 1998, frequency = 12)
dc.test.ts = ts(dc.test, start = 2023, frequency = 12)

time.pts = c(1:length(dc.train))
time.pts = c(time.pts - min(time.pts)) / max(time.pts)
gam.fit = gam(dc.train.ts ~ s(time.pts))
temp.fit.gam = ts(fitted(gam.fit), start = 1998, frequency = 12)
ts.plot(dc.train.ts, col="blue", main="DC spline", ylab = "DC price")
lines(temp.fit.gam, col = "red")

resid = resid(gam.fit)

#select order by aic
n = length(resid)
norder = 6 
p = c(1:norder) - 1
q = c(1:norder) - 1
aic <- matrix(0, norder, norder)
for (i in 1:norder) {
for (j in 1:norder) {
modij <- arima(resid, order = c(p[i], 0, q[j]), method = 'ML')
aic[i, j] <- modij$aic - 2 * (p[i] + q[j] + 1) + 2 * (p[i] + q[j] + 1) *
n / (n - p[i] - q[j] - 2)
}
}
aicv <- as.vector(aic)
plot(aicv, ylab = "AIC values")

indexp <- rep(c(1:norder), norder)
indexq <- rep(c(1:norder), each = norder)
indexaic <- which(aicv == min(aicv))
porder <- indexp[indexaic] - 1
qorder <- indexq[indexaic] - 1
final_model <- arima(resid, order = c(porder, 0, qorder), method = 'ML')
par(mfrow = c(2, 2))
plot(resid(final_model), ylab = 'Std Residuals')
abline(h = 0)
acf(as.vector(resid(final_model)), main = 'Residuals ACF')
pacf(as.vector(resid(final_model)), main = 'Residuals PACF')
qqnorm(resid(final_model))
qqline(resid(final_model))

#Test for Uncorrelated Residuals for the final model
Box.test(final_model$resid, lag = (porder + qorder + 1), type = "Box-Pierce", fitdf = (porder + qorder))
Box.test(final_model$resid, lag = (porder + qorder + 1), type = "Ljung-Box", fitdf = (porder + qorder))
jarque.bera.test(resid(final_model))
shapiro.test(resid(final_model))

final_model_pred <- arima(dc.train.ts, order = c(porder, 0, qorder),
method = 'ML')
arima_pred <- as.vector(predict(final_model_pred, n.ahead = 9))

dc.test.ts
arima_pred

#95% Prediction Intervals
arima_pred$pred[1] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred$se[1]

arima_pred$pred[2] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred$se[2]
```

*Response*







## Question 1c

Perform a similar analysis as in (1b) but this time applying ARIMA to the DC time series training dataset. Assume p,q belong to [0,5] and d belongs to [0,1]. Compare the forecast and discuss why these are different or similar from the testing data. Also evaluate the prediction intervals with a 95% confidence level, and compare the results of the forecast with the analysis in Question 1b by overlaying the ARMA and ARIMA predictions on the observed data.

```{r}
test_model = function(p, d, q) {
mod = arima(dc.train.ts, order = c(p, d, q), method = "ML")
current.aic = AIC(mod)
df = data.frame(p, d, q, current.aic)
names(df) = c("p", "d", "q", "AIC")
# print(paste(p, d, q, current.aic, sep = " "))
return(df)
}
orders = data.frame(Inf, Inf, Inf, Inf)
names(orders) = c("p", "d", "q", "AIC")
for (p in 0:5) {
for (d in 0:1) {
for (q in 0:5) {
possibleError = tryCatch(
orders = rbind(orders, test_model(p, d, q)),
error = function(e) {}
)
if (inherits(possibleError, "error"))
next
}
}
}
orders = orders[order(-orders$AIC), ]
tail(orders)

```


```{r}
arima.dc = arima(dc.train.ts, order = c(4, 1, 2), method = 'ML')
arima_pred1 = as.vector(predict(arima.dc, n.ahead = 2))
#### Test for Uncorrelated Residuals for the final model
Box.test(arima.gdp$resid,lag = (4 + 2 + 1), type = "Box-Pierce", fitdf = (4 + 2))
```
```{r}
jarque.bera.test(resid(arima.dc))
```
```{r}
shapiro.test(resid(arima.dc))
```
```{r}
arima_pred1
dc.test.ts
```

```{r}
# 95% Prediction Intervals
arima_pred1$pred[1] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred1$se[1]

arima_pred1$pred[2] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred1$se[2]
```

*Response*












# Question 2: Multivariate Analysis using VAR modeling

For this question, divide the entire average electricity data (all variables) into training data (excluding the last 9 months of 2023) and testing data (the last 9 of 2023). You will apply the modeling to the training data, and we will forecast the test data.

## Question 2a

Apply the VAR model to the multivariate time series including all 4 states' average electricity data. (Note that you will apply VAR to the training data.) Identify the VAR order using both AIC and BIC and compare (use a lag max of 10). If the selected order using AIC is larger than the selected order than selected using BIC, apply the Wald test to evaluate whether a smaller order than the one selected with AIC would be a better choice, meaning the smaller order model would perform similarly than the larger order model. Interpret the order selection.

This can be done by following the below substeps:- 1)Combine the variables into a multivariate dataset 2) select/display var orders and isolate models using AIC and BIC orders 3)Isolate coefficients and covariances from the AIC model 4) applying the Wald test to the values obtained from the AIC model, but would not be present in the BIC model. You can run a single Wald test for each variable, with all the lagged coefficients for that particular variable, in all resulting in 4 wald tests.

```{r}
la.train = la.price.ts[0:(length(la.price.ts) - 9)]
sea.train = sea.price.ts[0:(length(sea.price.ts) - 9)]
atl.train = atl.price.ts[0:(length(atl.price.ts) - 9)]

la.test = la.price.ts[(length(la.price.ts) - 8):length(la.price.ts)]
sea.test = sea.price.ts[(length(sea.price.ts) - 8):length(sea.price.ts)]
atl.test = atl.price.ts[(length(atl.price.ts) - 8):length(atl.price.ts)]

```

```{r}
la.train.ts = ts(la.train, start = 1998, frequency = 12)
sea.train.ts = ts(sea.train, start = 1998, frequency = 12)
atl.train.ts = ts(atl.train, start = 1998, frequency = 12)

la.test.ts = ts(la.test, start = 2023, frequency = 12)
sea.test.ts = ts(sea.test, start = 2023, frequency = 12)
atl.test.ts = ts(atl.test, start = 2023, frequency = 12)

```

```{r}
train.ts = cbind(dc.train.ts, la.train.ts, sea.train.ts, atl.train.ts)

test.ts = cbind(dc.test.ts, la.test.ts, sea.test.ts, atl.test.ts)

comp.aic <- VARselect(train.ts)

comp.aic$selection
```
```{r}
model.comp.aic <- VAR(train.ts, p = 3)
model.comp.bic <- VAR(train.ts, p = 2)
```

```{r}
pord.1 = model.comp.aic$p

coef.dc.2to3 = coefficients(model.comp.aic)$dc.train.ts[6:(3 * pord.1), 1]
coef.la.2to3 = coefficients(model.comp.aic)$la.train.ts[6:(3 * pord.1), 1]
coef.sea.2to3 = coefficients(model.comp.aic)$sea.train.ts[6:(3 * pord.1), 1]
coef.atl.2to3 = coefficients(model.comp.aic)$atl.train.ts[6:(3 * pord.1), 1]

## Covariance matrix of the coefficients
index.dc = 7:(5 * pord.1 + 1)
var.dc.2to3 = vcov(model.comp.aic)[index.dc, index.dc]

index.la = c(((5 * pord.1) + 13):(10 * pord.1 + 2))
var.la.2to3 = vcov(model.comp.aic)[index.la, index.la]

index.sea = c(((10 * pord.1) + 14):(15 * pord.1 + 3))
var.sea.2to3 = vcov(model.comp.aic)[index.sea, index.sea]

index.atl = c(((15 * pord.1) + 15):(15 * pord.1 + 4))
#var.atl.2to3 = vcov(model.comp.aic)[index.atl, index.atl]

```

```{r}
#wald.test(b = coef.dc.2to3, var.dc.2to3, Terms = seq(1, 5 * (pord.1 - 3)))
```
```{r}
#wald.test(b = coef.la.2to3, var.la.2to3, Terms = seq(1, 5 * (pord.1 - 5)))
```
```{r}
#wald.test(b = coef.sea.2to3, var.sea.2to3, Terms = seq(1, 5 * (pord.1 - 5)))
```
```{r}
#wald.test(b = coef.atl.3to7, var.atl.3to7, Terms = seq(1, 5 * (pord.1 - 5)))
```

*Response*














## Question 2b

Based on the analysis in 2a, select the VAR order using BIC and fit that model. Print out the model summary and comment on the statistical significance of the coefficients. Apply a model selection analysis using stepwise regression to select the models for each individual time series. What do you conclude from this model selection? Apply the restrict() command in R to restrict the model of order. How do the restricted models compare?

Follow the below steps to implement this: 1)Analyze the coefficients of the unrestricted model. 2)Then treat each of the series separately and do a stepwise regression by apply lm() and step() (in the backward direction with 5 steps) functions to each of the component time series to examine the coefficients 3)The stepwise regressions will return four separate models. Analyze if the same coefficients are significant in the overall VAR versus each of the stepwise models. Discuss. 4)Then build a model using restrict and see which predictors were significant in the restricted VAR model.
```{r}
summary(model.comp.bic)
```

```{r}
model_1 = step(lm(y~.,data = model.comp.bic$varresult$la.train.ts$model),
direction = "backward",steps = 3)
```


```{r}
model_2 = step(lm(y~.,data = model.comp.bic$varresult$sea.train.ts$model),
direction = "backward",steps = 3)
```
```{r}
model_3 = step(lm(y~.,data = model.comp.bic$varresult$sea.train.ts$model),
direction = "backward",steps = 3)
```
```{r}
model_4 <- step(lm(y~.,data = model.comp.bic$varresult$atl.train.ts$model),
direction = "backward",steps = 3)
```

```{r}
model.bic.restrict = restrict(model.comp.bic)
```


*Response*











## Question 2c

Evaluate the goodness of fit for the restricted BIC model using the multivariate ARCH test, the Jarque-Bera test and the Portmanteau test. State which assumptions are satisfied, and which are violated. (Note: While we evaluate the residuals for the normality assumption, we do not necessarily assume normality of the data. We use the normality assumption if we use the t-test to evaluate statistical significance.)

```{r}
arch.test(model.bic.restrict)
```

```{r}
normality.test(model.bic.restrict)
```

```{r}
serial.test(model.bic.restrict)
```
*Response*









## Question 2d

Using the VAR model with the order selected using BIC, forecast the last 9 months of 2023 of the DC data using the unrestricted and restricted VAR. Compare the predictions to the observed data. (You don't need to plot them (but can if you'd like). Using mean absolute percentage error and the precision measure, compare the predictions for DC results reran for 6 months from the univariate analysis model in 1c and this multivariate analysis. Discuss the differences or similarities.

```{r}
pred.unres = as.vector(predict(model.comp.bic, n.ahead = 9))
pred.res = as.vector(predict(model.comp.bic.restrict, n.ahead = 9))
#MAPE for unrestricted VAR
preds.unres = pred.unres$fcst$dc.train.ts[, 1]
obs = as.numeric(dc.test.ts)
100 * mean(abs(preds.unres - obs) / obs)

```

```{r}
#MAPE for restricted VAR
preds.res = pred.res$fcst$dc.train.ts[, 1]
obs = as.numeric(dc.test.ts)
100 * mean(abs(preds.res - obs) / obs)
```

```{r}
preds.arima = arima_pred1$pred
obs = as.numeric(dc.test.ts)
#    100 * mean(abs(preds.arima - obs) / obs)
```


```{r}
### Precision Measure (PM) for unrestricted VAR
sum((preds.unres - obs) ^ 2) / sum((obs - mean(obs)) ^ 2)
```

```{r}
### Precision Measure (PM) for restricted VAR
sum((preds.res - obs) ^ 2) / sum((obs - mean(obs)) ^ 2)
```

```{r}
### Precision Measure (PM) for restricted VAR
sum((preds.res - obs) ^ 2) / sum((obs - mean(obs)) ^ 2)
```

```{r}
### Precision Measure (PM) for ARIMA
#          sum((preds.arima - obs) ^ 2) / sum((obs - mean(obs)) ^ 2)
```


```{r}
# 95% Prediction Intervals and forcests
pred.unres$fcst$dc.train.ts
```

```{r}
pred.res$fcst$dc.train.ts
```

*Response*








## Question 2e

Perform a Granger Causality analysis using Wald test to evaluate whether any of state-level rate leads DC's average electricity price. Would any of the indicators help in predicting or explaining DC's electricity prices in the future? Provide your interpretation based on the Granger causality as well as for forecasting comparison in (2d). For this, use the unrestricted bic model from Question 2a.

For the Terms term in wald.test, you want to select all lags of the same state for evaluation (ex. For CHI-HOU test: the sequence should select coefficients chi.hou1, chi.houl2, chi.houl3, ..., chi.hou6).

```{r}
## Granger Causality: Wald Test
coef.dc = coefficients(model.comp.bic)$dc.train.ts[-(5 * 2 + 1), 1]
var.model = vcov(model.comp.bic)[2:11, 2:11]
#      wald.test(b = coef.dc, var.model, Terms = seq(2, 5 * 2, 5))
```
```{r}
#   wald.test(b = coef.dc, var.model, Terms = seq(3, 5 * 2, 5))
```
```{r}
#       wald.test(b = coef.dc, var.model, Terms = seq(4, 5 * 2, 5))
```


```{r}
#             wald.test(b = coef.dc, var.model, Terms = seq(5, 5 * 2, 5))
```

*Response*






















# Question 3

For this question, consider the training data to include the time values up to July 2016 and the testing data to include August 2016 to April 2017.

## Question 3a

Apply the VAR modeling approach with the order selected using the BIC approach giving the unrestricted VAR model. Apply a model selection analysis using stepwise regression to select the models for each individual time series. Based on the selected models, form the restricted VAR model, much like what was presented in the Moose R example code. Compare these two models in terms of coefficients and their statistical significance with the models derived in Question 2.

Follow the below steps to implement this: 1)Analyze the coefficients of the unrestricted model. 2)Then treat each of the series separately and do a stepwise regression by apply lm() and step()(in the backward direction with 2 steps) functions to each of the component time series to examine the coefficients 3)The stepwise regressions will return three separate models. Analyze if the same coefficients are significant in the overall VAR versus each of the stepwise models. Discuss. 4)Then build a model using restrict and see which predictors were significant in the restricted VAR model.



























