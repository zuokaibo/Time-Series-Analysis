---
title: "hw8"
output: html_document
date: "2025-04-11"
---

```{r setup, include=FALSE}
# Set up the default parameters
# 1. The code block will be shown in the document
# 2. set up figure display size
# 3. turn off all the warnings and messages

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 4)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```
# Background

We have explored how various U.S. economic indicators are related to each other, which is a classic application for the VAR modeling. The U.S. Federal Reserve use these economic indicators to make monetary policy decision. Specifically, we will focus on one the top commodities, electricity prices (cents per kilowatt-hour (cents/kWh), for the top most metropolitan areas in the U.S. In this homework, we will analysis the average electricity prices for the following areas:

-   Washington DC
-   Atlanta, GA
-   Los Angeles, CA
-   Seattle, WA

The data covered for the analysis is the period starting from Jan 1998 - Dec 2023, in a monthly frequency.

# Instructions on reading the data

To read the data in R, save the file in your working directory (make sure you have changed the directory if different from the R working directory) and read the data using the R function read.csv()

```{r cars}
# Read in the monthly data

data <- read.csv('Metro_electricity_prices.csv', header = TRUE)
idx <- as.Date(as.character(data$DATE), '%m/%d/%y')

```


```{r pressure, echo=FALSE}
library(lubridate)
library(dplyr)
library(data.table)
library(vars)
library(xts)
library(mgcv)
library(stats)
library(tseries)
library(aod)
library(forecast)
library(ggplot2)
```
# Question 1: Univariate Analysis

## Question 1a

Plot the time series of all indicators for comparison and discuss whether you find any similarities in terms of trend or other features.
Plot also the 1st order difference overlay plot and the corresponding ACF plots for the 4 states. Comment on the original and differenced time series data in terms of stationarity and volatility.

```{r}
# transform variables into time series
dc = xts(data$DC, idx)
la = xts(data$LA, idx)
sea = xts(data$SEA, idx)
atl = xts(data$ATL, idx)

# merge all univariate time series into multivariate time series 
all.merge = merge(dc, la, join="inner")
all.merge = merge(all.merge, sea, join = "inner")
all.merge = merge(all.merge, atl, join = "inner")
colnames(all.merge) = c("tsdc", "tsla", "tssea", "tsatl")

#plot time series and its first order differencing
plot(all.merge, main="price of four states")

all.merge.diff = diff(all.merge)
plot(all.merge.diff, main = "price of four states with first order differencing")

#plot ACF 
dc.price.ts = ts(as.numeric(all.merge$tsdc), start=c(1998,1), frequency = 12)
dc.price.diff = diff(dc.price.ts)
acf(dc.price.diff, main="DC price")

la.price.ts = ts(as.numeric(all.merge$tsla), start=c(1998,1), frequency = 12)
la.price.diff = diff(la.price.ts)
acf(la.price.diff, main="LA price")

sea.price.ts = ts(as.numeric(all.merge$tssea), start=c(1998,1), frequency = 12)
sea.price.diff = diff(sea.price.ts)
acf(sea.price.diff, main="Seattle price")

atl.price.ts = ts(as.numeric(all.merge$tsatl), start=c(1998,1), frequency = 12)
atl.price.diff = diff(atl.price.ts)
acf(atl.price.diff, main="Atlanta price")

```

*Response*
From original data, all indicators have an increasing trend until Jan2009, then a sudden drop, but a general increasing trend again till 2014, a sudden drop happened again then increased back. we can learn that the variability have increased or decreased dramaticlly over the time, even worse since 2019 due to covid. From the 1st order differencing data, we have removed the trend, but a strong volatilties still exist. The ACF plots of all time series have show a seasonality. 



## Question 1b

Using the original, undifferenced data, divide the data into training & test datasets with the last 9 months of 2023 being the testing data: April 2023 - Dec 2023. Fit a non-parametric trend and seasonality (use monthly factor) using the splines regression on the DC rate training dataset. Comment on the effectiveness of the splines fit. Then, apply ARMA to the residuals obtained from this splines fitting. Use max order of 6. Evaluate goodness of fit for the ARMA model (plot residuals, ACF, PACF, and Q-Q plot & residual tests).

Lastly, fit the ARMA model on training data (Not residuals) with the optimal orders selected above. Forecast the last 9 months of 2023 (testing data) and compare to the observed values. Discuss why there are (or not!) significant differences between predicted vs observed. To do this, you should also evaluate the prediction intervals with a 95% confidence level.

```{r}
dc.train = dc.price.ts[0:(length(dc.price.ts) - 9)]
dc.test = dc.price.ts[(length(dc.price.ts) - 8) : length(dc.price.ts)]

dc.train.ts = ts(dc.train, start = 1998, frequency = 12)
dc.test.ts = ts(dc.test, start = 2023, frequency = 12)

time.pts = c(1:length(dc.train))
time.pts = c(time.pts - min(time.pts)) / max(time.pts)
gam.fit = gam(dc.train.ts ~ s(time.pts))
temp.fit.gam = ts(fitted(gam.fit), start = 1998, frequency = 12)
ts.plot(dc.train.ts, col="blue", main="DC spline", ylab = "DC price")
lines(temp.fit.gam, col = "red")

resid = resid(gam.fit)

#select order by aic
n = length(resid)
norder = 6 
p = c(1:norder) - 1
q = c(1:norder) - 1
aic <- matrix(0, norder, norder)
for (i in 1:norder) {
for (j in 1:norder) {
modij <- arima(resid, order = c(p[i], 0, q[j]), method = 'ML')
aic[i, j] <- modij$aic - 2 * (p[i] + q[j] + 1) + 2 * (p[i] + q[j] + 1) *
n / (n - p[i] - q[j] - 2)
}
}
aicv <- as.vector(aic)
plot(aicv, ylab = "AIC values")

indexp <- rep(c(1:norder), norder)
indexq <- rep(c(1:norder), each = norder)
indexaic <- which(aicv == min(aicv))
porder <- indexp[indexaic] - 1
qorder <- indexq[indexaic] - 1
final_model <- arima(resid, order = c(porder, 0, qorder), method = 'ML')
par(mfrow = c(2, 2))
plot(resid(final_model), ylab = 'Std Residuals')
abline(h = 0)
acf(as.vector(resid(final_model)), main = 'Residuals ACF')
pacf(as.vector(resid(final_model)), main = 'Residuals PACF')
qqnorm(resid(final_model))
qqline(resid(final_model))

#Test for Uncorrelated Residuals for the final model
Box.test(final_model$resid, lag = (porder + qorder + 1), type = "Box-Pierce", fitdf = (porder + qorder))
Box.test(final_model$resid, lag = (porder + qorder + 1), type = "Ljung-Box", fitdf = (porder + qorder))
jarque.bera.test(resid(final_model))
shapiro.test(resid(final_model))

final_model_pred <- arima(dc.train.ts, order = c(porder, 0, qorder),
method = 'ML')
arima_pred <- as.vector(predict(final_model_pred, n.ahead = 9))

dc.test.ts
arima_pred

#95% Prediction Intervals
arima_pred$pred[1] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred$se[1]

arima_pred$pred[2] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred$se[2]
```

*Response*







## Question 1c

Perform a similar analysis as in (1b) but this time applying ARIMA to the DC time series training dataset. Assume p,q belong to [0,5] and d belongs to [0,1]. Compare the forecast and discuss why these are different or similar from the testing data. Also evaluate the prediction intervals with a 95% confidence level, and compare the results of the forecast with the analysis in Question 1b by overlaying the ARMA and ARIMA predictions on the observed data.

```{r}
test_model <- function(p, d, q) {
mod = arima(dc.train.ts, order = c(p, d, q), method = "ML")
current.aic = AIC(mod)
df = data.frame(p, d, q, current.aic)
names(df) <- c("p", "d", "q", "AIC")
# print(paste(p, d, q, current.aic, sep = " "))
return(df)
}
orders <- data.frame(Inf, Inf, Inf, Inf)
names(orders) <- c("p", "d", "q", "AIC")
for (p in 0:5) {
for (d in 0:1) {
for (q in 0:5) {
possibleError <- tryCatch(
orders <- rbind(orders, test_model(p, d, q)),
error = function(e) {}
)
if (inherits(possibleError, "error"))
next
}
}
}
orders <- orders[order(-orders$AIC), ]
tail(orders)

```


```{r}
arima.dc <- arima(dc.train.ts, order = c(4, 1, 2), method = 'ML')
arima_pred1 <- as.vector(predict(arima.dc, n.ahead = 2))
#### Test for Uncorrelated Residuals for the final model
Box.test(
arima.gdp$resid,
lag = (4 + 2 + 1),
type = "Box-Pierce",
fitdf = (4 + 2)
)
```
```{r}
jarque.bera.test(resid(arima.dc))
```
```{r}
shapiro.test(resid(arima.dc))
```
```{r}
arima_pred1
dc.test.ts
```

```{r}
# 95% Prediction Intervals
arima_pred1$pred[1] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred1$se[1]

arima_pred1$pred[2] + c(-1, 1) * qnorm((1 - 0.05 / 2)) * arima_pred1$se[2]
```

*Response*












# Question 2: Multivariate Analysis using VAR modeling

For this question, divide the entire average electricity data (all variables) into training data (excluding the last 9 months of 2023) and testing data (the last 9 of 2023). You will apply the modeling to the training data, and we will forecast the test data.

## Question 2a

Apply the VAR model to the multivariate time series including all 4 states' average electricity data. (Note that you will apply VAR to the training data.) Identify the VAR order using both AIC and BIC and compare (use a lag max of 10). If the selected order using AIC is larger than the selected order than selected using BIC, apply the Wald test to evaluate whether a smaller order than the one selected with AIC would be a better choice, meaning the smaller order model would perform similarly than the larger order model. Interpret the order selection.

This can be done by following the below substeps:- 1)Combine the variables into a multivariate dataset 2) select/display var orders and isolate models using AIC and BIC orders 3)Isolate coefficients and covariances from the AIC model 4) applying the Wald test to the values obtained from the AIC model, but would not be present in the BIC model. You can run a single Wald test for each variable, with all the lagged coefficients for that particular variable, in all resulting in 4 wald tests.











